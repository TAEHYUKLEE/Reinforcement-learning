{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b5a36791b804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcritic_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tfe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m   \u001b[1;31m# Externally in opensource we must enable exceptions to load the shared object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import critic_net\n",
    "import actor_net\n",
    "import Environment as En\n",
    "#from typing import List\n",
    "\n",
    "environment = En.env() #call environment\n",
    "#환경을 부른다. (사실 Environment라는 객체를 만든다)\n",
    "\n",
    "_ = environment.reset() #환경을 초기화한다.\n",
    "\n",
    "alpha_critic = 0.9 #learning rate (based on Q)\n",
    "#alpha_actor = 0.9\n",
    "\n",
    "#Input & Output\n",
    "state_nums = environment.state_num() #Q function은 (action, state) 이 두개에 의해 결정이 되므로, action까지 넣어줘야 한다.\n",
    "action_nums = environment.action_setting() #하나의 action 구성 list 개수를 의미한다.\n",
    "\n",
    "input_size_critic_s = state_nums\n",
    "input_size_critic_a = action_nums #일단 network는 나눠서 줄거니까.\n",
    "output_size_critic = 1 #DQN과 다르게 모든 action에 대한 Q를 계산하는 것이 아니라, Policy가 최적의 action을 선택해주기때문에 \n",
    "                       #하나의 Q값이 생긴다 (여느 state에 대한 최적의 action에 대해서)\n",
    "input_size_actor_s = state_nums\n",
    "output_size_actor =  action_nums\n",
    "\n",
    "\n",
    "#Reinforcement learning parmeter\n",
    "dis = 0.99  \n",
    "buffer_memory = 50000 #Replay memory에 몇개를 넣을 것인가? (Buffer)\n",
    "batch_size = 100 #Mini batch size Buffer에서 몇개씩 batch로 만들어서 학습시킬 것인가?\n",
    "\n",
    "\n",
    "def critic_train(main_critic, target_critic, main_actor, target_actor, train_batch):\n",
    "   # 학습시킬 Network와 데이터 batch가 배달옴\n",
    "    Q_old = np.zeros([1], dtype = np.float64) \n",
    "    Q_new = np.zeros([1], dtype = np.float64)\n",
    "    \n",
    "    x_action_stack = np.empty(0)\n",
    "    x_state_stack = np.empty(0)\n",
    "    y_stack = np.empty(0)\n",
    "    \n",
    "    x_state_stack = np.reshape(x_state_stack, (0, main_critic.input_size_critic_s))\n",
    "    x_action_stack = np.reshape(x_action_stack, (0, main_critic.input_size_critic_a))\n",
    "    y_stack = np.reshape(y_stack, (0, target_critic.output_size_critic)) #output_size_critic = 1로되어있다.\n",
    "\n",
    "    for state, action_noise, reward, next_state, done in train_batch: #이 부분들 다시 한 번 보도록 하자 (3번째 Cell에 연습함)\n",
    "\n",
    "        # Q = main_critic.predict(state, action) #이부분 state와 action을 같이 넣고 싶은데 문제가 생길듯.\n",
    "        # state, action_noise같이 넣어도 됨 그렇게 critic을 바꾸어 놓았다. (원래는 np.reshape이용해서 한 번에 합쳐서 보냈었음)\n",
    "        \n",
    "        #Q_old (prediction)\n",
    "        Q_old = main_critic.predict(state, main_actor) \n",
    "        \n",
    "        #next_state_action또한 정해줘야 한다 - target policy를 이용하여 next_state에 대한걸 넣어 next_action을 유추.\n",
    "        next_action = target_actor.predict(next_state) #target policy로 next_state에 대한 next_action을 받아온다.\n",
    "        \n",
    "        if done:\n",
    "            Q_new = reward\n",
    "        else:\n",
    "            Q_new = Q_old + alpha_critic*(reward + dis*(target_critic.predict(next_state, target_actor)) - Q_old)\n",
    "            \n",
    "        y_stack = np.vstack([y_stack, Q_new])\n",
    "        x_state_stack = np.vstack([x_state_stack, state]) #state를 학습시키는거지 Q를 학습시키는건 아니다.\n",
    "\n",
    "        x_action_stack = np.vstack([x_action_stack, action_noise])\n",
    "        \n",
    "        #actor에 input을 같이 줘야한다.\n",
    "        loss_critic, _ = main_critic.update(x_state_stack, y_stack, main_actor)\n",
    "        \n",
    "    return loss_critic, Q_old, Q_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def actor_train(main_actor, target_actor, main_critic, train_batch):\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    x_stack_actor = np.zeros([input_size_actor_s], dtype = np.float64)\n",
    "    x_stack_Q = np.zeros([input_size_critic_s], dtype = np.float64)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch: #이 부분들 다시 한 번 보도록 하자 (3번째 Cell에 연습함)\n",
    "\n",
    "        #actor_var = main_actor.predict(state)\n",
    "        actor_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"main_actor\")\n",
    "\n",
    "        x_stack = np.vstack([x_stack_actor, state]) \n",
    "        \n",
    "        Objective_Q = main_critic.Q_pred\n",
    "        \n",
    "        _ = main_actor.update(main_critic, x_stack)\n",
    "        \n",
    "        #서로 연결되어 있어서 main_actor update하기 위해 main_critic에 X_input를 넣어줘야한다\n",
    "        #InvalidArgumentError: You must feed a value for placeholder tensor \n",
    "        #'main_critic/input_critic_state' with dtype float and shape [?,180]\n",
    "        \n",
    "    return _   \n",
    "\n",
    "\n",
    "\n",
    "def copy_var_ops(*, target_scope_name =\"target\", main_scope_name = \"main\"):\n",
    "\n",
    "    op_holder = []\n",
    "\n",
    "    main_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=main_scope_name)\n",
    "    target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=target_scope_name)\n",
    "    \n",
    "    \n",
    "    for main_var, target_var in zip(main_vars, target_vars): \n",
    "        op_holder.append(target_var.assign(main_var.value()))\n",
    "        #dest_var(tensor). assign\n",
    "        \n",
    "    return op_holder\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    Q_old = np.empty(0)\n",
    "    Q_new = np.empty(0)  \n",
    "    st_step = 1 #action을 몇 time-step마다 취할 것인지에 대한 숫자\n",
    "    state_step = 0\n",
    "    record_frequency = 3\n",
    "    step_deadline = 1000\n",
    "    main_update_freq = 3\n",
    "    target_update_frequency = 3\n",
    "    #main이 target을 향해서 update되어가고 이후에 target_update가 이루어져야 하기때문에 main_freq < target_update가 되어야 한다.\n",
    "    max_episodes = 100\n",
    "    \n",
    "    # Replay buffer를 deque로 짠다. \n",
    "    buffer = deque() \n",
    "    #Memory는 50000개까지 \n",
    "\n",
    "    reward_buffer = deque() #maxlen=100\n",
    "    #reward_buffer또한 deque로 만들어서 마지막 100개까지 기억하도록 한다\n",
    "    \n",
    "    reward_record = open(\"reward.plt\" , 'w', encoding='utf-8', newline='') \n",
    "    reward_record.write('VARIABLES = \"Episode\", \"Reward\" \\n') \n",
    "    #Reward를 기록하기 위함.\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #formation of network for actor net\n",
    "        main_actor = actor_net.actor(sess, input_size_actor_s, output_size_actor, output_size_critic, name=\"main_actor\") \n",
    "        #TypeError: __init__() missing 1 required positional argument: 'output_size_critic' (main_actor에서)\n",
    "        target_actor = actor_net.actor(sess, input_size_actor_s, output_size_actor, output_size_critic, name=\"target_actor\")\n",
    "        \n",
    "        \n",
    "        #formation of network for critic net (first error NameError - input_size ciritic 등)\n",
    "        main_critic = critic_net.critic(sess, input_size_critic_s,input_size_critic_a, output_size_critic, main_actor.action_pred,  name=\"main_critic\") \n",
    "        target_critic = critic_net.critic(sess, input_size_critic_s,input_size_critic_a, output_size_critic, target_actor.action_pred, name=\"target_critic\")     \n",
    "        #main_actor.action_pred를 줌으로써 이어줘 본다.\n",
    "        \n",
    "        _ = main_actor.initialization(main_critic.Q_pred)\n",
    "        _ = target_actor.initialization(target_critic.Q_pred)\n",
    "    \n",
    "        sess.run(tf.global_variables_initializer()) #initializer <여기서 전체적으로 초기화해준다.>\n",
    "        print(\"initialization complete\")\n",
    "        \n",
    "        #Critic (copy)\n",
    "        copy_ops_critic = copy_var_ops(target_scope_name=\"target_critic\",main_scope_name=\"main_critic\")\n",
    "        sess.run(copy_ops_critic)\n",
    "        \n",
    "        #Policy (copy)\n",
    "        copy_ops_actor = copy_var_ops(target_scope_name=\"target_actor\", main_scope_name=\"main_actor\")\n",
    "        #sess.run(copy_ops_actor)\n",
    "        \n",
    "\n",
    "\n",
    "        for episode in range(0, max_episodes+1):\n",
    "            \n",
    "            print(\"Episode : {} start \".format(episode))\n",
    "        \n",
    "            done = False\n",
    "            \n",
    "            ##################### environment로부터 state를 받아온다 (observation) ###############\n",
    "            state = environment.reset() #envrionment로부터 state를 가져온다. (초기 state)\n",
    "            \n",
    "            reward_graph = 0\n",
    "            \n",
    "            \n",
    "            ############### 두개의 Neural network로 학습을 시키는 부분이다 ##########\n",
    "               \n",
    "            #정확히는 Episode 10이 끝난 시점에서 update하는 것이다.\n",
    "            \n",
    "            if episode > main_update_freq and episode % main_update_freq == 0: # train every 10 episodes\n",
    "                print(\"update start\")\n",
    "                loss_avg = 0\n",
    "                \n",
    "                for _ in range(50):\n",
    "                    #print(\"random_sample, step :{}\" ,format(_)) #check complete\n",
    "                    minibatch = random.sample(buffer, batch_size) \n",
    "                    minibatch = list(minibatch)\n",
    "                    \n",
    "                    #print(\"critic update start\")\n",
    "                    loss_critic, Q_old, Q_new= critic_train(main_critic, target_critic, main_actor, target_actor, minibatch)\n",
    "                    \n",
    "                    #print(\"actor update start\")\n",
    "                    _ = actor_train(main_actor, target_actor, main_critic, minibatch)\n",
    "                    \n",
    "                    loss_avg = loss_critic/50 +loss_avg\n",
    "                    \n",
    "                print(\"Loss for critic is : {}\".format(loss_avg))   \n",
    "                print(\"update end\")\n",
    "            ########################################################################\n",
    "            \n",
    "            if episode % record_frequency == 0:\n",
    "                record = environment.record_start(episode)\n",
    "                \n",
    "                \n",
    "                \n",
    "            while not done: #이부분이 한 episode에서 state를 진행시키는 부분 actor critic을 이 부분에 넣어야함.\n",
    "                \n",
    "                #Noise 매 step마다 Noise의 정도는 작아지게 설정할 것이다.\n",
    "                Noise = np.random.rand(1) # 매 step마다 Normal distribution에서 임의로 추출한다.\n",
    "                N = Noise*0.0001 / ((state_step / 10) + 1) #갈수록 Noise는 작아지게\n",
    "                \n",
    "                ##################### deterministic policy에서 state를 주고 action을 뽑아온다 ###################\n",
    "                action = main_actor.predict(state)\n",
    "                action_noise = action + N #with N(Noise) #180개를 받아온다. \n",
    "                #Noise 조절이 좀 필요하다 step loop 안으로 들어와야 계속 변할수 있다. \n",
    "                action_noise = np.reshape(action_noise, (input_size_critic_a))\n",
    "                \n",
    "                #action_noise = np.transpose(action_noise) # [180, 1] <--이런식으로 받아오게 되므로 reshape해야함\n",
    "                #print(action_noise.shape)\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                print(\"action_noise\")\n",
    "                print(action_noise)\n",
    "                print(\"type\")\n",
    "                print(type(action_noise))\n",
    "                print(action_noise.shape)\n",
    "                '''\n",
    "                #여기는 actor가 잘 돌아가는지 확인하는 부분. tensorflow는 trainable 초기화는 항상 해주고 \n",
    "                #placeholder에 X_input등을 넣어주면된다 (feed) 근데 여기서는 state가 input이니까 나오는게 맞음 feed안해도\n",
    "                \n",
    "                # Get new state and reward from environment  \n",
    "                next_state, reward, done, record = environment.simulation(action_noise, st_step, record)\n",
    "                \n",
    "                #한 step의 reward씩 계속 reward_graph에 쌓는다. summation of reward\n",
    "                reward_graph = reward + reward_graph\n",
    "                \n",
    "                if done:  \n",
    "                    reward = -1\n",
    "\n",
    "                ################ 이 부분이 Replay memory 부분이다 ##############\n",
    "                buffer.append((state, action_noise, reward, next_state, done))\n",
    "                if len(buffer) > buffer_memory:\n",
    "                    buffer.popleft()\n",
    "                \n",
    "                #main을 target으로 복사한다 (critic)\n",
    "                if state_step > 1 and state_step % target_update_frequency == 0:\n",
    "                    sess.run(copy_ops_critic)\n",
    "                    \n",
    "                #main을 target으로 복사한다 (actor)\n",
    "                if state_step > 1 and state_step % target_update_frequency == 0:\n",
    "                    sess.run(copy_ops_actor)\n",
    "  \n",
    "                #print(\"update out\")   \n",
    "                ################################################################       \n",
    "               \n",
    "                state = next_state\n",
    "                \n",
    "                state_step = state_step + 1\n",
    "                #print(\"step num : {}\".format(step))\n",
    "                \n",
    "                if state_step == step_deadline:\n",
    "                    break\n",
    "       \n",
    "            reward_graph = reward_graph/state_step\n",
    "            \n",
    "            #plt file로 reward graph 저장\n",
    "            reward_record.write(\"%d %f \\n\" %(episode , reward_graph))\n",
    "        \n",
    "            state_step = 0\n",
    "            #print(\"Episode : {} end \".format(episode))\n",
    "            \n",
    "           \n",
    "            if episode % record_frequency == 0:\n",
    "                _ = environment.record_end(record)\n",
    "            # Episode (finish)\n",
    "            \n",
    "    reward_record.close()\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #여기가 main 맞으니까 main이 실행되는거 맞는데 굳이 위와 같이 할 필요가 있나 싶은데?\n",
    "    main()\n",
    "    \n",
    "   \n",
    "    print(\"All process is finished!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
